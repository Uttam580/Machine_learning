{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\uttam\\anaconda3\\envs\\envpytorch\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\uttam\\anaconda3\\envs\\envpytorch\\lib\\site-packages (from torchvision) (1.16.2)\n",
      "Requirement already satisfied: six in c:\\users\\uttam\\anaconda3\\envs\\envpytorch\\lib\\site-packages (from torchvision) (1.15.0)\n",
      "Requirement already satisfied: torch in c:\\users\\uttam\\anaconda3\\envs\\envpytorch\\lib\\site-packages (from torchvision) (1.5.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\uttam\\anaconda3\\envs\\envpytorch\\lib\\site-packages (from torchvision) (5.4.1)\n",
      "Requirement already satisfied: future in c:\\users\\uttam\\anaconda3\\envs\\envpytorch\\lib\\site-packages (from torch->torchvision) (0.18.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of C:\\Users\\Uttam\\anaconda3\\envs\\envpytorch\\lib\\site-packages\\vision-1.0.0-py3.7-nspkg.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\Uttam\\anaconda3\\envs\\envpytorch\\lib\\site.py\", line 168, in addpackage\n",
      "      exec(line)\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"<frozen importlib._bootstrap>\", line 580, in module_from_spec\n",
      "  AttributeError: 'NoneType' object has no attribute 'loader'\n",
      "\n",
      "Remainder of file ignored\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST('', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = datasets.MNIST('', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: \n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterating over the dataset \n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([7, 1, 4, 7, 1, 5, 2, 6, 4, 2])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data[0][0],data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d13639ffd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(data[0][0].view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking label for output , output is balanced or not \n",
    "total = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in trainset:\n",
    "    Xs, ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "        total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.871666666666666%\n",
      "1: 11.236666666666666%\n",
      "2: 9.93%\n",
      "3: 10.218333333333334%\n",
      "4: 9.736666666666666%\n",
      "5: 9.035%\n",
      "6: 9.863333333333333%\n",
      "7: 10.441666666666666%\n",
      "8: 9.751666666666667%\n",
      "9: 9.915000000000001%\n"
     ]
    }
   ],
   "source": [
    "for i in counter_dict:\n",
    "    print(f\"{i}: {counter_dict[i]/total*100.0}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn #torch.nn gives access to neural network things like  layers\n",
    "import torch.nn.functional as F # give access to function like relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1) # finally layer we will use softmax \n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2667, -2.2828, -2.4511, -2.2084, -2.2470, -2.2474, -2.4460, -2.1914,\n",
       "         -2.4084, -2.3175]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#activation functions are keeping our data scaled between 0 and 1.\n",
    "# just an example.\n",
    "t1= torch.rand((28,28))\n",
    "t1= t1.view(-1,28*28)\n",
    "output = net(t1)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# loss function and optimiser\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1650'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)# setting gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0191, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2043, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3): # 3 full passes over the data\n",
    "    for data in trainset:  # `data` is a batch of data\n",
    "        X, y = data  # X is the batch of features, y is the batch of targets.\n",
    "        net.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\n",
    "        output = net(X.view(-1,784))  # pass in the reshaped batch (recall they are 28x28 atm)\n",
    "        loss = F.nll_loss(output, y)  # calc and grab the loss value\n",
    "        loss.backward()  # apply this loss backwards thru the network's parameters\n",
    "        optimizer.step()  # attempt to optimize weights to account for loss/gradients\n",
    "    print(loss)  # print loss. We hope loss (a measure of wrong-ness) declines! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1,784))\n",
    "        #print(output)\n",
    "        for idx, i in enumerate(output):\n",
    "            #print(torch.argmax(i), y[idx])\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.966\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(X[0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[0].view(-1,784))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.3555e+01, -1.0113e+01, -9.6664e+00, -1.0477e+01, -1.9269e+01,\n",
      "        -1.6346e+01, -3.1398e+01, -1.3327e-04, -1.3836e+01, -1.5979e+01],\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "a_featureset = X[0]\n",
    "reshaped_for_network = a_featureset.view(-1,784) # 784 b/c 28*28 image resolution.\n",
    "output = net(reshaped_for_network) #output will be a list of network predictions.\n",
    "first_pred = output[0]\n",
    "print(first_pred)\n",
    "biggest_index = torch.argmax(first_pred)\n",
    "print(biggest_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dig_recogniser_param.pt has saved successfully\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a1ed47498dda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mMODEL2\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'dig_recogniser.pt'\u001b[0m \u001b[1;31m# save with entire net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{MODEL1} has saved successfully\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMODEL1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMODEL2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "MODEL1  = 'dig_recogniser_param.pt' # save only with parameter \n",
    "MODEL2= 'dig_recogniser.pt' # save with entire net\n",
    "print(f\"{MODEL1} has saved successfully\")\n",
    "torch.save(net.state_dict(), MODEL1)\n",
    "torch.save(net, MODEL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "MODEL = 'dig_recogniser.pt'\n",
    "\n",
    "\n",
    "model = torch.load(MODEL)\n",
    "print('model loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Acer\n",
      " Volume Serial Number is 1023-C329\n",
      "\n",
      " Directory of C:\\Users\\Uttam\\Desktop\\Ds\\pytorch\n",
      "\n",
      "04-08-2020  01:00    <DIR>          .\n",
      "04-08-2020  01:00    <DIR>          ..\n",
      "03-08-2020  01:25    <DIR>          .ipynb_checkpoints\n",
      "28-07-2020  19:01             6,745 dia.pt\n",
      "28-07-2020  18:12            23,873 diabetes.csv\n",
      "28-07-2020  19:07           773,534 diabetes.ipynb\n",
      "03-08-2020  17:35           241,316 dig_recogniser.pt\n",
      "03-08-2020  17:35           238,092 dig_recogniser_param.pt\n",
      "01-08-2020  20:19            60,933 HousePrice.pt\n",
      "01-08-2020  20:19            35,064 HouseWeights.pt\n",
      "03-08-2020  15:32             4,856 img.jpg\n",
      "01-08-2020  20:24           119,039 kn_T5_House_price_prediction.ipynb\n",
      "02-08-2020  13:31    <DIR>          MNIST\n",
      "27-07-2020  19:12               305 pytorch_env.yml\n",
      "04-08-2020  01:00            31,941 sd_mnist .ipynb\n",
      "28-07-2020  19:45            18,098 torch_basic.ipynb\n",
      "              12 File(s)      1,553,796 bytes\n",
      "               4 Dir(s)  89,777,987,584 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'img.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_loader(loader, image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = loader(image).float()\n",
    "    image = torch.tensor(image, requires_grad=True)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
