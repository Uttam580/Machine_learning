{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "activation_and_loss_fn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7RALF9fLVZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGrQKmsYnlLr",
        "colab_type": "text"
      },
      "source": [
        "### Resources: \n",
        "\n",
        "https://gombru.github.io/2018/05/23/cross_entropy_loss/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwiH-a6-Lkru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#torch.cuda.is_available()"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Zb6NgOELk1V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e258d667-c40f-47ec-8fce-57c4e9e467b1"
      },
      "source": [
        "torch.cuda.device(0)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.cuda.device at 0x7fbdd04ca240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSPnz0hnMhJ4",
        "colab_type": "text"
      },
      "source": [
        "### Softmax \n",
        "softmax applies the exponential function to each element, and normalizes by dividing by the sum of all these exponentials\n",
        "* squashes the output to be between 0 and 1 = probability sum of all probabilities is 1\n",
        "* sigmoid squashes a vector in the range (0, 1)\n",
        "\n",
        "![image.png](https://i.stack.imgur.com/0rewJ.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDF8w-6jN2rO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcMH9XidLk47",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1908f4e-dbce-4e59-a5e0-9412e2a5931b"
      },
      "source": [
        "x = np.array([2.0, 1.0, 0.1])\n",
        "outputs = softmax(x)\n",
        "print('softmax numpy:', outputs)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "softmax numpy: [0.65900114 0.24243297 0.09856589]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYbEvtErOb01",
        "colab_type": "text"
      },
      "source": [
        "![img.png](https://i.ytimg.com/vi/lvNdl7yg4Pg/maxresdefault.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11K4_99kPddR",
        "colab_type": "text"
      },
      "source": [
        "### Cross entropy\n",
        "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. \n",
        "\n",
        "* loss increases as the predicted probability diverges from the actual label.\n",
        " So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.\n",
        "\n",
        " ![img.png](https://ml-cheatsheet.readthedocs.io/en/latest/_images/cross_entropy.png)\n",
        "\n",
        " The graph above shows the range of possible loss values given a true observation (isDog = 1). As the predicted probability approaches 1, log loss slowly decreases. As the predicted probability decreases, however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predictions that are confident and wrong!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NLqhQoROISP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy(actual, predicted):\n",
        "    EPS = 1e-15\n",
        "    predicted = np.clip(predicted, EPS, 1 - EPS)\n",
        "    loss = -np.sum(actual * np.log(predicted))\n",
        "    return loss # / float(predicted.shape[0])"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkSLLPPRWgKj",
        "colab_type": "text"
      },
      "source": [
        "![image.png](https://cdn-images-1.medium.com/max/1000/1*lUUmNbjMNS1rfX4El9i5VA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1PK_ArcoR8P",
        "colab_type": "text"
      },
      "source": [
        "![image.png](https://gombru.github.io/assets/cross_entropy_loss/intro.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZtY5QDXb8ZH",
        "colab_type": "text"
      },
      "source": [
        "### Example 1: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seWaQMgZZlN8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "eb93a36b-2c25-4996-e693-567e04a15f18"
      },
      "source": [
        "# y must be one hot encoded\n",
        "# if class 0: [1 0 0]\n",
        "# if class 1: [0 1 0]\n",
        "# if class 2: [0 0 1]\n",
        "Y = np.array([1, 0, 0])\n",
        "Y_pred_good = np.array([0.7, 0.2, 0.1])\n",
        "Y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
        "l1 = cross_entropy(Y, Y_pred_good)\n",
        "l2 = cross_entropy(Y, Y_pred_bad)\n",
        "print(f'Loss1 numpy: {l1:.4f}')\n",
        "print(f'Loss2 numpy: {l2:.4f}')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss1 numpy: 0.3567\n",
            "Loss2 numpy: 2.3026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJDZjwC4cEoI",
        "colab_type": "text"
      },
      "source": [
        "### Example: 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3ABpccbZlWQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c17855d5-3c42-4c39-fd4c-102c121deeb4"
      },
      "source": [
        "print('NN1')\n",
        "# defined three samples\n",
        "# actual cat , dog prediction \n",
        "y_actual1 = np.array([1,0])\n",
        "y_actual2 = np.array([0,1])\n",
        "y_actual3 = np.array([1,0])\n",
        "# predicted cat and dog prediction \n",
        "y_hat1= np.array([0.9,0.1])\n",
        "y_hat2= np.array([0.1,0.9])\n",
        "y_hat3 = np.array([0.4,0.6])\n",
        "ce1  = cross_entropy(y_actual1,y_hat1)\n",
        "ce2  = cross_entropy(y_actual2,y_hat2)\n",
        "ce3  = cross_entropy(y_actual3,y_hat3)\n",
        "avg_ce_loss = (ce1+ce2+ce3)/3\n",
        "print(f'Total cross entrophy loss for nn1 :{avg_ce_loss}')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN1\n",
            "Total cross entrophy loss for nn1 :0.37567058772993583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9xU5ZQZeiA6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b63fa1c7-feef-4b28-9210-ef79529422bf"
      },
      "source": [
        "print('NN2')\n",
        "# defined three samples\n",
        "# actual cat , dog prediction \n",
        "y_actual1 = np.array([1,0])\n",
        "y_actual2 = np.array([0,1])\n",
        "y_actual3 = np.array([1,0])\n",
        "# predicted cat and dog prediction \n",
        "y_hat1= np.array([0.6,0.4])\n",
        "y_hat2= np.array([0.3,0.7])\n",
        "y_hat3 = np.array([0.1,0.9])\n",
        "ce1  = cross_entropy(y_actual1,y_hat1)\n",
        "ce2  = cross_entropy(y_actual2,y_hat2)\n",
        "ce3  = cross_entropy(y_actual3,y_hat3)\n",
        "avg_ce_loss = (ce1+ce2+ce3)/3\n",
        "print(f'Total cross entrophy loss for nn2 :{avg_ce_loss}')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN2\n",
            "Total cross entrophy loss for nn2 :1.056695220232923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxm-kgrjcJ88",
        "colab_type": "text"
      },
      "source": [
        "![image.png](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/76_blog_image_7.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFYtZzFjZlXy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c883f8d0-d2fa-4ea1-dcec-2ba51a788ea4"
      },
      "source": [
        "# CrossEntropyLoss in PyTorch (applies Softmax)\n",
        "# nn.LogSoftmax + nn.NLLLoss\n",
        "# NLLLoss = negative log likelihood loss\n",
        "loss = nn.CrossEntropyLoss()\n",
        "# loss(input, target)\n",
        "\n",
        "# target is of size nSamples = 1\n",
        "# each element has class label: 0, 1, or 2\n",
        "# Y (=target) contains class labels, not one-hot\n",
        "Y = torch.tensor([0])\n",
        "print(Y)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCW_svmRZlbL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "a72cc66d-e4ee-4325-80bb-0660db66802c"
      },
      "source": [
        "# input is of size nSamples x nClasses = 1 x 3\n",
        "# y_pred (=input) must be raw, unnormalizes scores (logits) for each class, not softmax\n",
        "Y_pred_good = torch.tensor([[2.0, 1.0, 0.1]])\n",
        "Y_pred_bad = torch.tensor([[0.5, 2.0, 0.3]])\n",
        "l1 = loss(Y_pred_good, Y)\n",
        "l2 = loss(Y_pred_bad, Y)\n",
        "\n",
        "print(f'PyTorch Loss1: {l1.item():.4f}')\n",
        "print(f'PyTorch Loss2: {l2.item():.4f}')\n",
        "\n",
        "# get predictions\n",
        "_, predictions1 = torch.max(Y_pred_good, 1)\n",
        "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
        "print(f'Actual class: {Y.item()}, Y_pred1: {predictions1.item()}, Y_pred2: {predictions2.item()}')"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch Loss1: 0.4170\n",
            "PyTorch Loss2: 1.8406\n",
            "Actual class: 0, Y_pred1: 0, Y_pred2: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz9Ad3OGZlej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "69d69a70-df12-4245-aeff-a09395434ede"
      },
      "source": [
        "# allows batch loss for multiple samples\n",
        "\n",
        "# target is of size nBatch = 3\n",
        "# each element has class label: 0, 1, or 2\n",
        "Y = torch.tensor([2, 0, 1])\n",
        "\n",
        "# input is of size nBatch x nClasses = 3 x 3\n",
        "# Y_pred are logits (not softmax)\n",
        "Y_pred_good = torch.tensor(\n",
        "    [[0.1, 0.2, 3.9], # predict class 2\n",
        "    [1.2, 0.1, 0.3], # predict class 0\n",
        "    [0.3, 2.2, 0.2]]) # predict class 1\n",
        "\n",
        "Y_pred_bad = torch.tensor(\n",
        "    [[0.9, 0.2, 0.1], # will predict 0\n",
        "    [0.1, 0.3, 1.5],  # will predict 2\n",
        "    [1.2, 0.2, 0.5]]) # will predict 0\n",
        "\n",
        "l1 = loss(Y_pred_good, Y)\n",
        "l2 = loss(Y_pred_bad, Y)\n",
        "print(f'Batch Loss1:  {l1.item():.4f}')\n",
        "print(f'Batch Loss2: {l2.item():.4f}')\n",
        "\n",
        "# get predictions\n",
        "_, predictions1 = torch.max(Y_pred_good, 1) # retun  max along  dim=1, along rows\n",
        "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
        "print(f'Actual class: {Y}, Y_pred1: {predictions1}, Y_pred2: {predictions2}')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch Loss1:  0.2834\n",
            "Batch Loss2: 1.6418\n",
            "Actual class: tensor([2, 0, 1]), Y_pred1: tensor([2, 0, 1]), Y_pred2: tensor([0, 2, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q8agWlvkDpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Binary classification\n",
        "class NeuralNet1(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(NeuralNet1, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_size, 1)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.linear1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        # sigmoid at the end\n",
        "        y_pred = torch.sigmoid(out)\n",
        "        return y_pred"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aotcCBr0kLCS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "105d4091-dfe6-4a31-d1fb-98bd56a9c588"
      },
      "source": [
        "model = NeuralNet1(input_size=28*28, hidden_size=5)\n",
        "print(model)\n",
        "criterion = nn.BCELoss()\n",
        "print(criterion)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NeuralNet1(\n",
            "  (linear1): Linear(in_features=784, out_features=5, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (linear2): Linear(in_features=5, out_features=1, bias=True)\n",
            ")\n",
            "BCELoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7watSgaYkDsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Multiclass problem\n",
        "class NeuralNet2(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet2, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_size, num_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.linear1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        # no softmax at the end\n",
        "        return out"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mMBZmPgkUsZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "d0bad858-5f65-4e78-f22a-4bd529ea483e"
      },
      "source": [
        "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
        "print(model)\n",
        "criterion = nn.CrossEntropyLoss()  # (applies Softmax)\n",
        "print(criterion)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NeuralNet2(\n",
            "  (linear1): Linear(in_features=784, out_features=5, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (linear2): Linear(in_features=5, out_features=3, bias=True)\n",
            ")\n",
            "CrossEntropyLoss()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8GcLKTvzuBO",
        "colab_type": "text"
      },
      "source": [
        "### Activation functions\n",
        "\n",
        "* Activation are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated (“fired”) or not, based on whether each neuron’s input is relevant for the model’s prediction. \n",
        "* Activation functions also help normalize the output of each neuron to a range between 1 and 0 or between -1 and 1.\n",
        "\n",
        "#### Resources: \n",
        "https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/#:~:text=Activation%20functions%20are%20mathematical%20equations,relevant%20for%20the%20model's%20prediction.\n",
        "\n",
        "![img.png](https://missinglink.ai/wp-content/uploads/2018/11/Basic-process-carried-out-by-a-neuron-in-a-neural-network.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A4X_q9y6V3K",
        "colab_type": "text"
      },
      "source": [
        "#### Binary Step function \n",
        "\n",
        "A binary step function is a threshold-based activation function. If the input value is above or below a certain threshold, the neuron is activated and sends exactly the same signal to the next layer.\n",
        "\n",
        "![img.png](https://missinglink.ai/wp-content/uploads/2018/11/binarystepfunction.png)\n",
        "\n",
        "The problem with a step function is that it does not allow multi-value outputs—for example, it cannot support classifying the inputs into one of several categories.\n",
        "\n",
        "\n",
        "#### Linear Activation Function\n",
        "\n",
        "It takes the inputs, multiplied by the weights for each neuron, and creates an output signal proportional to the input. In one sense, a linear function is better than a step function because it allows multiple outputs, not just yes and no.\n",
        "\n",
        "![img.png](https://missinglink.ai/wp-content/uploads/2018/11/graphsright.png)\n",
        "\n",
        "However, a linear activation function has two major problems:\n",
        "\n",
        "1. Not possible to use backpropagation  (gradient descent) to train the model—the derivative of the function is a constant, and has no relation to the input, X. So it’s not possible to go back and understand which weights in the input neurons can provide a better prediction.\n",
        "\n",
        "\n",
        "2. All layers of the neural network collapse into one—with linear activation functions, no matter how many layers in the neural network, the last layer will be a linear function of the first layer (because a linear combination of linear functions is still a linear function). So a linear activation function turns the neural network into just one layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fIWtz-l8BFv",
        "colab_type": "text"
      },
      "source": [
        "#### Non Linear activation function.\n",
        "\n",
        "* They allow backpropagation because they have a derivative function which is related to the inputs.\n",
        "* They allow “stacking” of multiple layers of neurons to create a deep neural network. Multiple hidden layers of neurons are needed to learn complex data sets with high levels of accuracy.\n",
        "\n",
        "\n",
        "#### 1. Sigmoid\n",
        "\n",
        "* Advantages\n",
        "\n",
        "  Smooth gradient, preventing “jumps” in output values.\n",
        "\n",
        "  Output values bound between 0 and 1, normalizing the output of each neuron.\n",
        "\n",
        "\n",
        "* Disadvantages\n",
        "\n",
        " Vanishing gradient—for very high or very low values of X, there is almost no change to the prediction, causing a vanishing gradient problem. This can result in the network refusing to learn further, or being too slow to reach an accurate prediction.\n",
        "\n",
        " Outputs not zero centered.\n",
        "\n",
        " Computationally expensive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_EA6K6v_FV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# output = w*x + b\n",
        "# output = activation_function(output)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "x = torch.tensor([-1.0, 1.0, 2.0, 3.0])"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh1_NdLv_LQ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "440a42cb-6fda-49a0-f191-df1fbd9d59eb"
      },
      "source": [
        "# sofmax\n",
        "output = torch.softmax(x, dim=0)\n",
        "print(output)\n",
        "sm = nn.Softmax(dim=0)\n",
        "output = sm(x)\n",
        "print(output)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.0120, 0.0889, 0.2418, 0.6572])\n",
            "tensor([0.0120, 0.0889, 0.2418, 0.6572])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQfj972X-88X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "39667686-ca49-4b1b-cd7a-fe3468390eb3"
      },
      "source": [
        "# sigmoid \n",
        "output = torch.sigmoid(x)\n",
        "print(output)\n",
        "s = nn.Sigmoid()\n",
        "output = s(x)\n",
        "print(output)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.2689, 0.7311, 0.8808, 0.9526])\n",
            "tensor([0.2689, 0.7311, 0.8808, 0.9526])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6vZnFgm857x",
        "colab_type": "text"
      },
      "source": [
        "#### 2. TanH / Hyperbolic Tangent\n",
        "\n",
        "* Advantages\n",
        " Zero centered—making it easier to model inputs that have strongly negative, neutral, and strongly positive values.Otherwise like the Sigmoid function.\n",
        "\n",
        "* Disadvantages\n",
        "\n",
        " Like the Sigmoid function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUQFKm40_iD7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "667f124d-058d-478f-b844-6fc24f7c10da"
      },
      "source": [
        "#tanh\n",
        "output = torch.tanh(x)\n",
        "print(output)\n",
        "t = nn.Tanh()\n",
        "output = t(x)\n",
        "print(output)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.7616,  0.7616,  0.9640,  0.9951])\n",
            "tensor([-0.7616,  0.7616,  0.9640,  0.9951])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ5LYBD79NCg",
        "colab_type": "text"
      },
      "source": [
        "#### 3. ReLU (Rectified Linear Unit)\n",
        "\n",
        "* Advantages\n",
        "\n",
        " Computationally efficient—allows the network to converge very quickly\n",
        "Non-linear—although it looks like a linear function, ReLU has a derivative function and allows for backpropagation\n",
        "\n",
        "* Disadvantages\n",
        "\n",
        " The Dying ReLU problem—when inputs approach zero, or are negative, the gradient of the function becomes zero, the network cannot perform backpropagation and cannot learn.\n",
        "\n",
        "**In mathematics a function is considered linear whenever a fucntion f:A→B if for every x and y in the domain A has the following property: f(x)+f(y)=f(x+y). By definition the ReLU is max(0,x). Therefore, if we split the domain from (−∞,0] or [0,∞) then the function is linear. However, it's easy to see that f(−1)+f(1)≠f(0). Hence by definition ReLU is not linear**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TtSaVkG_mV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "01ab976b-c9b8-4bfc-c1b8-5f0b2274fb13"
      },
      "source": [
        "# relu\n",
        "output = torch.relu(x)\n",
        "print(output)\n",
        "relu = nn.ReLU()\n",
        "output = relu(x)\n",
        "print(output)\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 2., 3.])\n",
            "tensor([0., 1., 2., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TKYec7i9uTE",
        "colab_type": "text"
      },
      "source": [
        "#### 4. Leaky ReLU\n",
        "\n",
        "* Advantages\n",
        " Prevents dying ReLU problem—this variation of ReLU has a small positive slope in the negative area, so it does enable backpropagation, even for negative input values Otherwise like ReLU\n",
        "\n",
        "* Disadvantages\n",
        "\n",
        " Results not consistent—leaky ReLU does not provide consistent predictions for negative input values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKsWrBkb_q-S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7aa747ab-994d-4b29-f7c0-e37a324b814d"
      },
      "source": [
        "# leaky relu\n",
        "output = F.leaky_relu(x)\n",
        "print(output)\n",
        "lrelu = nn.LeakyReLU()\n",
        "output = lrelu(x)\n",
        "print(output)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.0100,  1.0000,  2.0000,  3.0000])\n",
            "tensor([-0.0100,  1.0000,  2.0000,  3.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zROwVqCK6WNC",
        "colab_type": "text"
      },
      "source": [
        "![img.png](https://miro.medium.com/max/1200/1*ZafDv3VUm60Eh10OeJu1vw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucg9P3y2-qTU",
        "colab_type": "text"
      },
      "source": [
        "![img.png](https://miro.medium.com/max/1600/1*n1HFBpwv21FCAzGjmWt1sg.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B6z47jS0aDP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "3503fa80-4415-4593-d7f1-544af2109f8e"
      },
      "source": [
        "#nn.ReLU() creates an nn.Module which you can add e.g. to an nn.Sequential model.\n",
        "#torch.relu on the other side is just the functional API call to the relu function,\n",
        "#so that you can add it e.g. in your forward method yourself.\n",
        "\n",
        "# option 1 (create nn modules)\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.linear1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "model = NeuralNet(input_size=28*28, hidden_size=5)\n",
        "model"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNet(\n",
              "  (linear1): Linear(in_features=784, out_features=5, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (linear2): Linear(in_features=5, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_ingGgm0aTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# option 2 (use activation functions directly in forward pass)\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.linear1(x))\n",
        "        out = torch.sigmoid(self.linear2(out))\n",
        "        return out"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-iAcufr0aWo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "88230503-8cfc-4637-c261-31557eea0817"
      },
      "source": [
        "model = NeuralNet(input_size=28*28, hidden_size=5)\n",
        "model"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNet(\n",
              "  (linear1): Linear(in_features=784, out_features=5, bias=True)\n",
              "  (linear2): Linear(in_features=5, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    }
  ]
}